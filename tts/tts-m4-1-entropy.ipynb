{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Speech Processing Labs 2020: TTS: Module 4_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this first\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Entropy\n",
    "\n",
    "\n",
    "### Learning Outcomes\n",
    "* Understand that entropy measures uncertainty\n",
    "* Gain some intutitions about how entropy behaves\n",
    "* See that entropy can be reduced by splitting a data set into two partitions\n",
    "\n",
    "### Need to know\n",
    "* Topic Videos: Decision tree, Learning decision trees\n",
    "\n",
    "Our goal in this sequence of notebooks is to understand how a classification tree is learned from data. \n",
    "\n",
    "Each split of the data in a decision tree decreases uncertainty about the value of the predictee: we become more and more certain of its value as we descend the tree. We can measure the amount of uncertainty using entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 How entropy is calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a categorical variable with M possible values, or *classes*.\n",
    "\n",
    "Entropy is defined as\n",
    "\n",
    "$$ \\Large H = - \\sum_{i=1}^{M} p_i log_2(p_i) $$\n",
    "\n",
    "where the $p_i$ are the probabilities of each of the $M$ classes. $H$ is the entropy in **bits**. It is a measure of uncertainty. Higher entropy means \"more unpredictable / higher uncertainty\". Lower entropy means \"more predictable / more certainty\". \n",
    "\n",
    "To help you inderstand the equation, here's a short video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "IPython.display.IFrame(width=\"640\",height=\"428\",src=\"https://fast.wistia.net/embed/iframe/utpd6km04m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here's a Python function to compute entropy from an array of counts, or probabilities. (It works for either case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(counts):\n",
    "    \"\"\" accepts an array of counts or probabilities and computes -1 * sum {p * log p}\"\"\"\n",
    "    H=0 # entropy\n",
    "    total_count=float(sum(counts))\n",
    "    for c in counts:\n",
    "        if c > 0: # cannot take log of zero\n",
    "            p=float(c)/total_count\n",
    "            H=H + p * math.log2(p)\n",
    "    H=H*-1.0\n",
    "    return H # in bits, because log was base 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Get an intuitive understanding of entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you visualise probability distributions, here's a function for plotting one. It also computes the entropy of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(labels,counts,title='Distribution'):\n",
    "    if sum(counts) == 0:\n",
    "        print(\"Cannot handle this case!\")\n",
    "        return 0\n",
    "    total_count=float(sum(counts))\n",
    "    pdf = [c / total_count for c in counts]\n",
    "    x_pos = [i for i, _ in enumerate(labels)]\n",
    "    plt.bar(x_pos, pdf, color='blue')\n",
    "    plt.title(title+\" (entropy={:.3} bits)\".format(entropy(counts)))\n",
    "    plt.xlabel(\"label\")\n",
    "    plt.ylabel(\"probability\")\n",
    "    plt.xticks(x_pos, labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 What entropy measures about a probability distribution\n",
    "\n",
    "Now find out by experimentation what the **highest and lowest values of entropy** are. The variable (which will be called the predictee when we build a Decision Tree) here is \"Fruit\" and it has two possible values (= classes) of \"Apple\" and \"Orange\". You are going to directly manipulate the count of each class in the code and see what the effect on the entropy is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the labels of the two classes (i.e, the values the categorical random variable \"Fruit\" can take)\n",
    "labels = ['Apple', 'Orange']\n",
    "\n",
    "# the number of examples of each class in our data set\n",
    "counts = [4, 10] # <- play with the distribution of counts\n",
    "\n",
    "plot_distribution(labels,counts,\"Fruit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Try different numbers of classes\n",
    "What is the relationship between the number of classes and the **highest value of entropy** you can acheive?\n",
    "(Hint: try with 2, 4, and 8 classes, as well as other numbers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add and remove classes to change how many there are\n",
    "labels = ['k', 's', 'ʃ', 'tʃ']\n",
    "\n",
    "# the number of counts must match the number of classes\n",
    "counts = [11180, 2185, 1170, 2005] # <- play with the distribution of counts\n",
    "\n",
    "# for example, how about a distribution over 5 classes\n",
    "labels = ['a', 'b', 'c', 'd', 'e']\n",
    "counts = [12,   45, 101,  22,  99] # <- play with the distribution of counts\n",
    "\n",
    "# or over 8 classes (or any other number - please experiment!)\n",
    "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "counts = [12,   45, 101,  22,  99,  17,   9,  4 ] # <- play with the distribution of counts\n",
    "\n",
    "plot_distribution(labels,counts,\"My distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go back to the equation and relate what you have found by experimentation to the terms in the equation. Where in the equation is the number of classes that you just varied? Where in the equation is the probability distribution over those classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Reduce entropy\n",
    "\n",
    "From your experiments above you should have learned that a uniform distribution has maximum entropy, and anything you do to make it more uneven will reduce entropy. The limit is reached when only one class has a non-zero count: then the entropy is zero.\n",
    "\n",
    "Our decision tree will be trying to reduce entropy. It seems that the way to do that is to make the probability distribution less uniform (more 'uneven').\n",
    "\n",
    "So, how can we make a probability distribution less uniform? Your next task is to take a distribution and split it into two distributions that have lower entropy. Try different ways to split the ditribution. How much can you reduce the entropy? Is it ever possible to *increase* the entropy?\n",
    "\n",
    "Here's the original distribution, over 4 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['k', 's', 'ʃ', 'tʃ'] # do not change this\n",
    "counts = np.array([11180, 2185, 1170, 2005]) # do not change this\n",
    "\n",
    "print(\"The distribution before the split was\",counts)\n",
    "print(\"and the entropy of that distribution is {:.3} bits\".format(entropy(counts)))\n",
    "plot_distribution(labels,counts,\"Original distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the above counts into two partitions. We'll call then 'left' and 'right' because we're eventually going to build a decision tree (not yet though!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play around with these values (they can't be larger than the original counts above though)\n",
    "left_counts = np.array([46, 1339, 12, 104])\n",
    "\n",
    "right_counts = np.subtract(counts,left_counts) # this is the remaining data; do not change this line\n",
    "\n",
    "print(\"The two distributions after the split are\",left_counts,\"and\",right_counts)\n",
    "print(\"Entropies of the two distributions are {:.3} bits and {:.3} bits.\".format(entropy(left_counts),entropy(right_counts)))\n",
    "\n",
    "# the total entropy after splitting is simply a weighted sum of the two entropies\n",
    "total_entropy = ( sum(left_counts)*entropy(left_counts) + sum(right_counts)*entropy(right_counts) ) / (sum(left_counts) + sum(right_counts))\n",
    "print(\"Total entropy of the two distributions is {:.3} bits \".format(total_entropy))\n",
    "print(\"which is a reduction of {:.3} bits compared to the original distribution.\".format(entropy(counts)-total_entropy))\n",
    "\n",
    "plot_distribution(labels,left_counts,\"Left partition\")\n",
    "plot_distribution(labels,right_counts,\"Right partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You should now understand how entropy behaves. As we make a probability distribution more and more predictable, entropy reduces. This will be the goal of our decision tree: to reduce uncertainty about the value of the predictee.\n",
    "\n",
    "In this notebook, when you split a distribution into two partitions, you should have been able to acheive large reductions in entropy, like in the \"ideal Decision Tree\" in the topic video [Learning Decision Trees](http://speech.zone/courses/speech-processing/module-4-speech-synthesis-front-end-2/videos/learning-decision-trees/).\n",
    "\n",
    "When we build our decision tree, we can only make splits based on questions about the *predictors*. This is because, at inference time, that is all we will know.\n",
    "\n",
    "We'll do that in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
