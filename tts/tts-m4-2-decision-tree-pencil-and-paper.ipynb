{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Speech Processing Labs 2020: TTS: Module 4_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this first\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Building a Decision Tree (using pencil and paper)\n",
    "\n",
    "### Learning Outcomes\n",
    "* Understand in detail how to choose the best partition of some data, to reduce entropy\n",
    "* Be able to calculate entropy for distributions of real data\n",
    "* Be able to grow a small decision tree without running any code\n",
    "\n",
    "### Need to know\n",
    "* what entropy is (from the previous notebook)\n",
    "* Topic Videos: Prosody, Decision tree, Learning decision trees\n",
    "\n",
    "**This exercise requires pencil and paper, so go get some! I mean it!**\n",
    "\n",
    "We are going to do a few iterations of growing a Decision Tree by hand. This is not a handwritten tree though: we're running the normal algorithm, but we're going to perform it manually so we really understand every step. (Later, we'll run it in code.)\n",
    "\n",
    "You should do this notebook in groups of 2-3 students.\n",
    "\n",
    "Before you start, here's a short video to remind you how a decision tree is learned, to complement the topic videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "IPython.display.IFrame(width=\"640\",height=\"428\",src=\"https://fast.wistia.net/embed/iframe/9l5iq63ezi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Raw data\n",
    "\n",
    "You are going to grow a small decision tree to perform the task of predicting where to place phrase breaks in a sentence, based only on the text. Fortunately, someone has already recorded some sentences, listened for where the speaker made phrase breaks, and labelled the data with those, so you don't need to. That's a relief! The raw data look like this, where \"|\" marks a phrase break:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "You know a little too much, | Mr Hannay. |\n",
    "It was obvious | that he was badly puzzled. |\n",
    "In half an hour | I was reading. |\n",
    "Then something awoke me. |\n",
    "He stammered a little, | like a man picking his words. |\n",
    "I pulled up a chair | and sat down on it. |\n",
    "I tore open the Tide Tables | and found Bradgate. |\n",
    "I took my head in my hands | and thought. |\n",
    "He came back | in ten minutes | with a long face. |\n",
    "He will be in London | at five. |\n",
    "Then the dark fell, | and silence. |\n",
    "But it was a chance, | the only possible chance. |\n",
    "It was obvious | that he was badly puzzled. |\n",
    "Then with some difficulty | I turned the car. |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data preparation (feature extraction and feature engineering)\n",
    "\n",
    "We rarely use the raw data directly in machine learning. Usually, we have some choices to make, using our linguistic knowledge and our skills as machine learning engineers.\n",
    "\n",
    "### 2.2.1 Dealing with variable length sequences\n",
    "As in so many problems involving a variable length sequence (here, the sentence), we reduce the problem to a fixed length sequence by using a sliding window. We need to do this because Decision Trees work with a fixed set of predictors, not a variable number.\n",
    "\n",
    "The sliding window will be placed around each location where a break might occur: each word juncture.\n",
    "\n",
    "### 2.2.2 Choosing the predictors\n",
    "We also have to choose whether to use the raw predictor values (the orthographic words), or to process them in some way. Words are an open set with a very large numer of possible values, so the data will be very sparse. We will replace each word with a super-simple Part-Of-Speech (POS) tag:\n",
    "\n",
    "    PUNC = punctuation\n",
    "    CONT = content words\n",
    "    FUNC = function words\n",
    "\n",
    "That makes the data look like this, stored as list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    \"FUNC CONT FUNC CONT CONT CONT PUNC | CONT CONT PUNC |\",\n",
    "    \"FUNC CONT CONT | FUNC FUNC CONT CONT CONT PUNC |\",\n",
    "    \"FUNC CONT FUNC CONT | FUNC CONT CONT PUNC |\",\n",
    "    \"FUNC CONT CONT FUNC PUNC |\",\n",
    "    \"FUNC CONT FUNC CONT PUNC | CONT FUNC CONT CONT FUNC CONT PUNC |\",\n",
    "    \"FUNC CONT FUNC FUNC CONT | FUNC CONT CONT FUNC FUNC PUNC |\",\n",
    "    \"FUNC CONT CONT FUNC CONT CONT | FUNC CONT CONT PUNC |\",\n",
    "    \"FUNC CONT FUNC CONT FUNC FUNC CONT | FUNC CONT PUNC |\",\n",
    "    \"FUNC CONT CONT | FUNC CONT CONT | FUNC FUNC CONT CONT PUNC |\",\n",
    "    \"FUNC CONT FUNC FUNC CONT | FUNC CONT PUNC |\",\n",
    "    \"FUNC FUNC CONT CONT PUNC | FUNC CONT PUNC |\",\n",
    "    \"FUNC CONT CONT FUNC CONT PUNC | FUNC CONT CONT CONT PUNC |\",\n",
    "    \"FUNC CONT CONT | FUNC FUNC CONT CONT CONT PUNC |\",\n",
    "    \"FUNC FUNC FUNC CONT | FUNC CONT FUNC CONT PUNC |\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 What is the predictee?\n",
    "The predictee always has to have a value. In the raw data, it is only defined at word junctures where there was a break. We will rewrite the predictee so it is either \"NB\" (no break) or \"B\" (break) at *every word juncture*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data prepared for machine learning\n",
    "\n",
    "### 2.3.1 Nicely formatted data\n",
    "\n",
    "From the raw data, we need to extract a set of data points for learning a decision tree. Each data point comprises the values of the predictors and the corresponding correct value of the predictee. This is the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "for line in corpus:\n",
    "    # pad, ready for sliding window\n",
    "    line=\"PAD \"+line+\" PAD\"\n",
    "    line=line.replace(\" | \",\"-B-\").replace(\" \",\" NB \").replace(\"-B-\",\" B \")\n",
    "    words=line.split()\n",
    "    for i in range(0,len(words)):\n",
    "        if words[i] == \"B\" or words[i] == \"NB\":\n",
    "            data.append((words[i-1],words[i+1],words[i]))\n",
    "\n",
    "print(\"Created\",len(data),\"data points. Each data point has the form (PREV, NEXT, PREDICTEE)\")\n",
    "for d in data:\n",
    "    print(\"{}\".format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 List all possible questions we can ask about the predictors\n",
    "\n",
    "There are two predictors - the POS of the previous and next word surrounding a word juncture. We've called them PREV and NEXT. Each can take one of a fixed set of values. Write down all possible questions you can ask, in the box below. Don't worry about whether they are *actually useful* - that's not your job! (The data will tell us that.) I've given you one question as a starting point:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "PREV==\"FUNC\"\n",
    "```\n",
    "...now edit this part to write down all other possible questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tree growing\n",
    "\n",
    "### 2.4.1 Measure the entropy of the data before it is split\n",
    "\n",
    "Do this by hand. You need to find the distribution of predictee values in the above data set, then apply the entropy equation. Do the calculation on paper and not in Python! The only part you can't easily do by hand is to compute logarithms, so here's a function for you to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.5 # adjust this as required\n",
    "print(\"{:.3}\".format(math.log2(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Pick a question and split the data, then measure the entropy of the two partitions\n",
    "\n",
    "I'll start you off, and give you some code that will split the data for you. If you prefer (and you might learn more by doing this), you could print out the data on paper and cut it into 145 pieces, then do all of this without any code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes=[]\n",
    "no=[]\n",
    "for d in data:\n",
    "    # d[0] contains PREV and d[1] contains NEXT, with the predictee in d[2]\n",
    "    if d[0] == \"FUNC\": # <- change this for each of your questions in turn\n",
    "        yes.append(d)\n",
    "    else:\n",
    "        no.append(d)\n",
    "\n",
    "yes_predictees = [d[2] for d in yes]\n",
    "no_predictees = [d[2] for d in no]\n",
    "\n",
    "print(\"Data for 'yes' is\",yes,\"\\n\")\n",
    "print(\"Data for 'no'  is\",no,\"\\n\\n\")\n",
    "yes_predictees.sort()\n",
    "no_predictees.sort()\n",
    "print(\"Predictee distribution for 'yes' is\",yes_predictees,\"\\n\")\n",
    "print(\"Predictee distribution for 'no'  is\",no_predictees,\"\\n\\n\")\n",
    "print(\"From a total of\",len(data),\"data points: 'yes' partition\",len(yes),\"/ 'no' partition\",len(no))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the entropy of each of those two distributions, and compute the weighted sum. This will give the total entropy of the partitioned data.\n",
    "\n",
    "The 'yes' one is easy - they are all 'NB' so the entropy is 0. You do the one for 'no'.\n",
    "\n",
    "#### Repeat for all questions\n",
    "\n",
    "Repeat step 2.4.2 for every one of of the questions you came up with in 2.3.2 (divide the labour amongst your group), recording the entropy for each one.\n",
    "\n",
    "### 2.4.3 Place the best question into the tree\n",
    "\n",
    "Once you have decided on the best question to split the whole data, write that as the root of the tree and draw a 'yes' and 'no' branch (really, do it on a piece of paper!). \n",
    "\n",
    "### 2.4.4 Recurse\n",
    "\n",
    "Now recurse down one or both of those branches, treating each partition in turn as the training data. Your goal is to grow a *very* small tree. Keep going until you think you properly understand the algorithm.\n",
    "\n",
    "## 2.5 Test\n",
    "\n",
    "When you are finished, make up a new sentence and use your tree to predict where the phrase breaks should be placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
